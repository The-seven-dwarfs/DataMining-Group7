from math import log, ceil
import shutil as shl
import pandas as pd
import numpy as np
from lingua import Language, LanguageDetectorBuilder
import multiprocessing as mp

def compute_whiskers(series: pd.Series)-> tuple:
    """
    Given a series of numerical values, returns the lower and upper bound of the
    whiskers generated by a boxplot.
    """
    Q1 = series.quantile(0.25)
    median = series.quantile(0.5)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5*IQR
    upper_bound = Q3 + 1.5*IQR
    return (lower_bound, upper_bound)


def get_inf_elements(series: pd.Series) -> pd.Series:
    """
    Given a series, return the subset of elements containt inf value
    """
    tmp = series.map(np.isinf, na_action="ignore")
    return series[tmp == True]
    
def get_sturges_bins(sample_size: int)->float:
	"""
	Given the sample size of a distribution, returns the number of bins 
	for an histogram rapresenting the distribution according to Sturges' rule 
	"""
	return ceil( log(sample_size,2) + 1 )

def unpack():
	shl.unpack_archive("dataset/users.zip", "dataset") # unpacks the users.zip into the datasets folder (The users zip is small enough to be commited to github if we would like)
	shl.unpack_archive("dataset/tweets.zip", "dataset") # unpacks the tweets.zip into the datasets folder

def repair_lang_attribute(users_df: pd.DataFrame):
	# wrong_fields = ["Select Language...", "xx-lc"] # only 3 elements
	to_map_fields = {
		"en-gb": "en-GB",
		"zh-tw": "zh-TW",
		"zh-cn": "zh-CN",
		"fil": "fil-PH"
	}

	# dropping wrong fields
	"""
	wrong_index = lambda x: True if x[1] in wrong_fields else False
	wrong_indexes = [index for (index, _) in filter(wrong_index, enumerate(users_df["lang"]))]
	users_df.drop(index=wrong_indexes, inplace=True)
	"""

	# mapping incorrect values to fixed ones
	for language in to_map_fields:
		indexes = users_df[users_df["lang"] == language].index
		for index in indexes:
			old_language = users_df.loc[index,"lang"]
			users_df.loc[index,"lang"] = to_map_fields[old_language]


def concurrent_language_model(dataset : pd.DataFrame, parallelism_level : int) -> pd.DataFrame:

	def process_function(datset_part_number : int, dataset_part : pd.DataFrame, return_queue : mp.Queue) -> pd.DataFrame:
		# from all language: all avaiable languages
		# with low accuracy: faster, without this it takes at least 1 second for every tweet (but even more if the text is long)
		# with preload language model: slower in building the model but faster in the prediction
		detector = LanguageDetectorBuilder.from_all_languages().with_low_accuracy_mode().with_preloaded_language_models().build()
		dataset_part["tweet_lang"] = [(detector.detect_language_of(text)) for text in dataset_part["text"]]
		return_queue.put( (dataset_part, datset_part_number) )
		print("Process number", datset_part_number, "finished")


	num_process = parallelism_level
	process_list = []
	splitted_dataset = np.array_split(dataset, num_process)
	concurrent_queue = mp.Queue()
	
	for i in range(len(splitted_dataset)):
		process_list.append(mp.Process(target=process_function, args=( i, splitted_dataset[i] , concurrent_queue,  )))

	for i in process_list:
		i.start()

	splitted_dataset = []

	print("All", num_process, "processes runnig ...")
	
	for i in range(len(process_list)):
		print("Trying to collect", i + 1, "/", len(process_list), "of information")
		splitted_dataset.append(concurrent_queue.get(block=True))

	print("All computation retreived, merging spitted datasets")

	splitted_dataset.sort(key = lambda x : x[1])
	splitted_dataset = ( i for (i, _) in splitted_dataset)
	dataset = pd.concat(splitted_dataset)
	dataset.reset_index(drop=True, inplace=True)

	print("Merging completed")
	return dataset

